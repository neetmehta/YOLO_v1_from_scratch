{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from os.path import join as osp\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "img_transforms = transforms.Compose([transforms.Resize((384,1248)), transforms.ToTensor()])\n",
    "\n",
    "label_map = {\n",
    "                'Car':                 0,\n",
    "                'Van':                 1,\n",
    "                'Truck':               2,\n",
    "                'Pedestrian':          3,\n",
    "                'Person_sitting':      4,\n",
    "                'Cyclist':             5,\n",
    "                'Tram':                6,\n",
    "                'Misc':                7,\n",
    "                'DontCare':            8\n",
    "            }\n",
    "\n",
    "class KittiDetection2D(Dataset):\n",
    "\n",
    "    def __init__(self, root, transforms=None):\n",
    "        super(KittiDetection2D, self).__init__()\n",
    "        self.image_dir = osp(root, \"image_2\")\n",
    "        self.label_dir = osp(root, \"label_2\")\n",
    "        self.image_list = os.listdir(osp(root, \"image_2\"))\n",
    "        self.label_list = os.listdir(osp(root, \"label_2\"))\n",
    "        self.transforms = transforms\n",
    "        self.h = 384\n",
    "        self.w = 1248\n",
    "        self.S = (11,24)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(osp(self.image_dir, self.image_list[index]))\n",
    "        label_pth = osp(self.label_dir, self.label_list[index])\n",
    "        target = torch.zeros(11, 24, 14)\n",
    "        with open(label_pth, 'r') as f:\n",
    "            for i in f.readlines():\n",
    "                obj_class, x1, y1, x2, y2 = self._parse_label(i)\n",
    "                cell, x, y, w, h = self._convert_label(x1, y1, x2, y2)\n",
    "                target[cell[0], cell[1]] = self._create_vector(obj_class, x, y, w, h)\n",
    "            \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def _create_vector(self, obj, x, y, h, w):\n",
    "        obj_vector = torch.zeros(9)\n",
    "        obj_vector[obj] = 1\n",
    "        return torch.cat((obj_vector, torch.tensor([1,x,y,h,w])))\n",
    "\n",
    "    def _parse_label(self, label):\n",
    "        label = label.split()\n",
    "        obj_class = label_map[label[0]]\n",
    "        x1, y1, x2, y2 = int(float(label[4])), int(float(label[5])), int(float(label[6])), int(float(label[7]))\n",
    "        return obj_class, x1, y1, x2, y2\n",
    "\n",
    "    def _convert_label(self, x1, y1, x2, y2):\n",
    "        x = int((x1+x2)/2)/self.w\n",
    "        y = int((y1+y2)/2)/self.h\n",
    "        h = int((y2-y1))/self.h\n",
    "        w = int((x2-x1))/self.w\n",
    "        cell = int(y*self.S[0]), int(x*self.S[1])\n",
    "        x = x*self.S[1]-cell[1]\n",
    "        y = y*self.S[0]-cell[0]\n",
    "        h = self.S[0]*h\n",
    "        w = self.S[1]*w\n",
    "        return cell,x,y,w,h\n",
    "\n",
    "ds = KittiDetection2D(r\"E:\\Deep Learning Projects\\datasets\\kitti_object_detection\\Kitti\\raw\\training\", img_transforms)\n",
    "x, target = ds[0]\n",
    "x.unsqueeze_(0)\n",
    "target.unsqueeze_(0)\n",
    "from model import YOLOv1\n",
    "from utils import read_yaml, iou\n",
    "\n",
    "model_cfg = read_yaml('model.yaml')\n",
    "model = YOLOv1(model_cfg)\n",
    "pred = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6]), tensor([14]))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.where(target[0,..., 9]==1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existbox tensor([1.])\n",
      "tensor([-0.0395,  0.0335,  0.0334, -0.0004], grad_fn=<SelectBackward0>) tensor([0.6346, 0.4453, 1.8846, 4.6979])\n",
      "tensor([-0.0395,  0.0335,  0.0334, -0.0004], grad_fn=<SelectBackward0>) tensor([-0.0395,  0.0335,  0.0334, -0.0004], grad_fn=<SelectBackward0>)\n",
      "tensor(0.1647, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "loss = MSELoss()\n",
    "tar_b = target[..., 10:14]\n",
    "exists_box = target[...,9:10]\n",
    "print('existbox', exists_box[0,6,14])\n",
    "pred_b1 = pred[...,10:14]\n",
    "pred_b2 = pred[..., 15:19]\n",
    "print(pred_b1[0,6,14], tar_b[0,6,14])\n",
    "iou_b1 = iou(pred_b1, tar_b).unsqueeze(-1)\n",
    "iou_b2 = iou(pred_b2, tar_b).unsqueeze(-1)\n",
    "_, best_box = torch.max(torch.cat((iou_b1,iou_b2), dim=-1), dim=-1)\n",
    "best_box.unsqueeze_(-1)\n",
    "pred_b = best_box*pred[..., 15:19] + (1-best_box)*pred[..., 10:14]\n",
    "print(pred_b1[0,6,14], pred_b[0,6,14])\n",
    "pred_coord = torch.sqrt(torch.abs(pred_b1[...,2:4]))\n",
    "target_coord = torch.sqrt(tar_b[...,2:4])\n",
    "coord_loss = 5*loss(torch.flatten(pred_coord), torch.flatten(target_coord))\n",
    "print(coord_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "cord loss tensor(0.0059, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0709, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yololoss = YoloLoss()\n",
    "yololoss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils import iou\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=(11,24), B=2, C=9, coord=5, noobj=0.5) -> None:\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.coord = coord\n",
    "        self.noobj = noobj\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        class_probs = target[..., :self.C]                              # [N, S[0], s[1], C]\n",
    "        exist_box_identity = target[..., self.C:self.C+1]\n",
    "        target_box = exist_box_identity * target[..., self.C+1:]                       # [N, S[0], s[1], 4]\n",
    "        print(target_box.unique())\n",
    "\n",
    "        iou_b1 = iou(pred[..., 10:14], target[...,10:14]).unsqueeze(-1) # [N, S[0], S[1], 1]\n",
    "        iou_b2 = iou(pred[...,15:19], target[..., 10:14]).unsqueeze(-1) # [N, S[0], S[1], 1]\n",
    "        max_iou, best_box = torch.max(torch.cat((iou_b1,iou_b2), dim=-1), dim=-1)            # best_box [N, S[0], S[1]]\n",
    "        best_box = best_box.unsqueeze(-1)\n",
    "        pred_best_box = exist_box_identity*(best_box*pred[...,15:19] + (1-best_box)*pred[..., 10:14]) # [N, S[0], s[1], 4]\n",
    "        \n",
    "        ## coord loss\n",
    "        pred_coord = pred_best_box[..., 0:2]\n",
    "        target_coord = target_box[..., 0:2]\n",
    "        coord_loss = self.coord*self.mse_loss(torch.flatten(pred_coord, 0, -2), torch.flatten(target_coord, 0, -2))\n",
    "\n",
    "        ## box loss\n",
    "        \n",
    "        pred_h_w = torch.sign(pred_best_box[...,2:4])*torch.sqrt(torch.abs(pred_best_box[..., 2:4])) # [N, S[0], S[1], 2]\n",
    "        target_h_w = torch.sqrt(target_box[..., 2:4]) #[N, S[0], S[1], 2]\n",
    "        box_loss = self.coord*self.mse_loss(torch.flatten(pred_h_w, 0, -2), torch.flatten(target_h_w, 0, -2))\n",
    "\n",
    "        ## object loss\n",
    "        pred_obj = exist_box_identity*(best_box*pred[...,14:15] + (1-best_box)*pred[..., 9:10])\n",
    "\n",
    "        object_loss = self.mse_loss(torch.flatten(pred_obj, 0, -2), torch.flatten(exist_box_identity, 0, -2))\n",
    "\n",
    "        ## no object loss\n",
    "\n",
    "        noobject_loss = self.mse_loss(torch.flatten((1-exist_box_identity)*pred[...,9:10], 0, -2), torch.flatten((1-exist_box_identity)*target[...,9:10], 0, -2)) + self.mse_loss(torch.flatten((1-exist_box_identity)*pred[...,9:10], 0, -2), torch.flatten((1-exist_box_identity)*target[...,9:10], 0, -2))      ###### CHECK\n",
    "        noobject_loss = self.noobj*object_loss\n",
    "\n",
    "        ## class loss\n",
    "        pred_class = exist_box_identity*pred[..., :9]\n",
    "        target_class = exist_box_identity*target[..., :9]\n",
    "\n",
    "        class_loss = self.mse_loss(torch.flatten(pred_class, 0, -2), torch.flatten(target_class, 0, -2))\n",
    "        print('cord loss', coord_loss)\n",
    "        # print('box loss', box_loss)\n",
    "        # print('obj loss', object_loss)\n",
    "        # print('noobj loss', noobject_loss)\n",
    "        # print('class loss', class_loss)\n",
    "\n",
    "        return box_loss + coord_loss + object_loss + noobject_loss + class_loss\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7199)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = YoloLoss()\n",
    "loss(torch.rand(1,11,24,19), torch.rand(1,11,24,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_b1 = iou(torch.rand(1,11,24,4), torch.rand(1,11,24,4)).unsqueeze(-1)\n",
    "iou_b2 = iou(torch.rand(1,11,24,4), torch.rand(1,11,24,4)).unsqueeze(-1)\n",
    "max_iou, best_box = torch.max(torch.cat((iou_b1,iou_b2), dim=-1), dim=-1)\n",
    "best_box.unsqueeze_(-1)\n",
    "pred_box = best_box * torch.rand(1,11,24,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 24, 4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1,11,24,4)[..., :5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 24, 4])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([264, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(torch.rand(1,11,24,4), 0,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1575035622ce78d0fdd034cce6fb6336dc5a68a5b98817405196dd053fee3f88"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
